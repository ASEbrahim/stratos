# STATE.md — StratOS Project State

> Living document. Updated at the end of every Claude Code session.
> Loaded into Claude.ai Project Knowledge and Claude Code context.
> **CLAUDE.md** = how to work here. **STATE.md** = what happened and why.

Last updated: **2026-02-26** (session: pipeline optimizations + retention system)

---

## Current State

### What's Deployed

- **Server**: `python3 main.py --serve --background` on port 8080
- **Active profile**: Ahmad — Petroleum Engineering Professor at Kuwait University / Seisnetics consultant
- **Context hash**: `8e86611559e6` (role + context + location)
- **Scorer model**: `stratos-scorer-v2` (8.7GB, LoRA fine-tuned)
- **Inference model**: `qwen3:30b-a3b` (~18GB) — strat agent + market analysis only
- **Wizard/briefing model**: `qwen3:14b` (~9GB) — wizard, briefings, profile generation
- **Search provider**: Serper (Google) with DuckDuckGo fallback
- **Serper credits**: ~2220 remaining of 2500 (key: `1b9db98...` in `.env`)

### Last Scan Output

- 100 articles, 3 retained, 8 categories populated
- Categories: oiltech (43), kuniv (34), confer (10), career (6), govpol (3), cybersec (2), cloudai (1), certev (1)
- Briefing: present (deferred, generated by 14b in ~15-30s)
- Incremental scans stable: output stays at 100 across back-to-back scans

### What's Working

- Full scan pipeline: fetch → score → defer briefing → output
- Incremental scanning: reuses scores from snapshot, carries forward unfetched articles
- Profile-scoped retention: context hash tags, filtered by active role
- Deferred briefing: scan completes immediately, briefing patches output async
- Model routing: 14b handles briefings (14-30s), 30b reserved for agent chat
- Two-pass scoring: fast timeout Pass 1, 3x slower Pass 2 for deferred items
- Serper + DDG dual provider with automatic fallback

### What's Not Yet Done

- **Cross-profile re-scoring** (Phase 2 from retention spec): when switching profiles, re-score other profiles' retained articles against the new context
- **Intelligence Archive** (Phase 3): dedicated searchable archive of all retained signals across all profiles
- **VRAM coexistence test**: scorer (8.7GB) + 14b (~9GB) may fit in 24GB simultaneously — haven't tested `OLLAMA_MAX_LOADED_MODELS=2` yet
- **Frontend SSE for briefing_ready**: backend broadcasts `briefing_ready` event, but frontend doesn't listen for it yet (briefing appears on next data poll, not instantly)
- **RSS feeds**: currently empty (`rss_feeds: []`), all news comes from search queries

---

## Decision Log

Append-only. Newest first. Format: `[date] decision — rationale`

### 2026-02-26

- **[2026-02-26] Carry forward unfetched snapshot articles in incremental scan** — The initial incremental scan implementation only reused scores for articles that were re-fetched AND found in the snapshot. Articles not re-fetched (because Serper dedup skipped the query) were dropped, shrinking output from 100 → 21. Fix: also carry forward snapshot articles not in the fetch set. Retained articles excluded (handled separately by `_merge_retained_articles`).

- **[2026-02-26] In-memory snapshot for incremental scoring, not database** — The original spec proposed a `scored_at` column in `news_items` + per-URL DB lookups. We used the existing `_snapshot_previous_articles()` data instead: zero migrations, zero DB changes, purely in-memory URL→score lookup. Covers the common case (24h window). Trade-off: doesn't persist across server restarts (snapshot is re-read from output JSON on next scan start, so it does survive restarts as long as the output file exists).

- **[2026-02-26] Deferred briefing as background thread, not async/await** — Python's `threading.Thread(daemon=True)` is simpler than asyncio for a single background task. The briefing thread reads/writes the output JSON file (patching in the briefing), which is atomic enough for a single-user system. Multi-user would need a lock.

- **[2026-02-26] Briefings on wizard_model (14b), not inference_model (30b)** — Briefings are summaries, not agent reasoning. 14b produces adequate briefings in 14-30s vs 2-5 min for 30b. The real win: scorer (8.7GB) + 14b (~9GB) = ~18GB, which may fit in 24GB VRAM simultaneously, eliminating model swap latency entirely. Reused the existing `wizard_model` config key rather than adding a fourth (`briefing_model`) — same model, less config.

- **[2026-02-26] Context hash (role+context+location) over profile name for retention** — A single profile can have multiple roles (Ahmad as CE student vs petroleum professor). Profile name doesn't capture the intelligence context. SHA-256 of normalized `role|context|location` does. Normalization (lowercase, collapse whitespace) prevents hash changes from trivial edits. 12-char hex prefix is sufficient for uniqueness. Old profile-name tags age out naturally via `retention_max_age_hours`.

- **[2026-02-26] `retained_by_profile` stored in output JSON, not in database** — Retention is an output-layer concern: articles are tagged during `_merge_retained_articles()` and persist through the JSON output cycle. No schema migration needed. The field name says "profile" but stores a hash — renaming would be unnecessary churn.

- **[2026-02-26] Serper API key exhaustion was the real bug, not env loading** — Initial diagnosis: `_load_env_secrets()` not called after config reload in `run_scan()`. This was a real code bug (fixed), but the actual reason scans used DDG exclusively was that the old Serper key (`ec4d2...`) had zero credits server-side despite the local tracker showing 202 remaining. Confirmed with direct curl. New key (`1b9db98...`) resolved it.

---

## Failure Log

Things that were tried and failed, to prevent repeating them. Only non-obvious entries.

### 2026-02-26

- **Serper local tracker vs server-side credits can desync** — The `SerperQueryTracker` counts queries locally, but Serper's server tracks credits independently. If the key is used from another machine or the dashboard, the local count becomes wrong. The 400 error `{"message":"Not enough credits"}` is the symptom. Fix: always trust the API response over local tracking. There's a `set_remaining()` method to sync, but it requires manual invocation.

- **`_build_output()` whitelist drops unknown fields** — The output builder constructs each article dict from an explicit key list. Any new field added to articles in-memory (like `retained_by_profile`) gets silently dropped unless also added to the whitelist. This cost a full debug cycle. Lesson: when adding article metadata, always check `_build_output()`.

### Earlier (from CLAUDE.md, preserved here for searchability)

- **AOTRITON on ROCm 6.2** — `TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1` causes NaN gradients on 7900 XTX. Do not enable.
- **Qwen3 `think: False`** — Qwen3 ignores this parameter and leaks reasoning into the content field, consuming the entire `num_predict` budget. Omit the parameter entirely; Ollama separates thinking automatically.
- **PEFT meta device** — After `get_peft_model()`, must delete `hf_device_map` and force `.to("cuda:0")` or gradient computation crashes.
- **Training v15-v18** — Multiple failed training runs. Key lessons: answer-only training (v18) didn't work; training format must be character-for-character identical to inference format in `scorer_adaptive.py`.

---

## Profiles

| Name | Role | Hash |
|------|------|------|
| Ahmad | Petroleum Engineering Professor / Seisnetics Consultant | `8e86611559e6` |
| saa | (incomplete/test profile) | — |

---

## Key File Locations

| What | Where |
|------|-------|
| Main orchestrator | `backend/main.py` |
| Scorer | `backend/processors/scorer_adaptive.py` |
| Briefing generator | `backend/processors/briefing.py` |
| News fetcher | `backend/fetchers/news.py` |
| Kuwait intelligence | `backend/fetchers/kuwait_scrapers.py` |
| Serper client | `backend/fetchers/serper_search.py` |
| Database | `backend/database.py` |
| Auth/profiles | `backend/auth.py`, `backend/profiles/*.yaml` |
| Server/routes | `backend/server.py`, `backend/routes/` |
| Output JSON | `backend/output/news_data.json` |
| API keys | `backend/.env` (gitignored) |
| Codebase guide | `backend/CLAUDE.md` |
| This file | `STATE.md` (repo root) |
