# STATE.md — StratOS Project State

> **Purpose:** Single source of truth for project context across Claude.ai and Claude Code sessions. Read this first. Updated at the end of every session.
>
> **Last updated:** 2026-02-26 (10-commit session: pipeline optimizations + retention system)
>
> **Separation of concerns:** `CLAUDE.md` = how to work with the codebase. `STATE.md` = what happened and why. `config.yaml` = what's configured now.

---

## 1. Current State

### What's Deployed

- **Server**: `python3 main.py --serve --background` on port 8080
- **Active profile**: Ahmad — Petroleum Engineering Professor at Kuwait University / Seisnetics consultant
- **Context hash**: `8e86611559e6` (SHA-256 of role|context|location)
- **Scorer**: `stratos-scorer-v2` — DoRA fine-tuned Qwen3-8B, Q8_0 GGUF (8.7GB). Production metrics: 98.1% direction accuracy, 0.393 MAE, 0.750 Spearman ρ, 7.90 profile awareness spread.
- **Agent/market model**: `qwen3:30b-a3b` (~18GB) — MoE 30B total / 3.3B active. Reserved for strat agent chat + market analysis only.
- **Wizard/briefing model**: `qwen3:14b` (~9GB) — wizard, briefings, profile generation
- **Search provider**: Serper (Google) with DuckDuckGo fallback
- **Serper credits**: ~2220 remaining of 2500 (key: `1b9db98...` in `.env`)

### Scan Pipeline Performance (as of Feb 26)

| Metric | Value |
|--------|-------|
| Full scan (cold) | ~5 min (scoring ~450s + fetch ~30s, briefing async) |
| Incremental scan | ~60s (scores reused, 0 LLM calls) |
| Briefing | 14-30s, non-blocking (background thread) |
| Articles per scan | ~100-148 |
| Scoring speed | ~3.3s/article |
| Output stability | Stable at 100 across incremental scans |

### Last Scan Output

- 100 articles, 3 retained, 8 categories populated
- Categories: oiltech (43), kuniv (34), confer (10), career (6), govpol (3), cybersec (2), cloudai (1), certev (1)
- Briefing: present (deferred, generated by 14b in ~15-30s)

### What's Working

- Full scan pipeline: fetch → score → defer briefing → output
- Incremental scanning: reuses scores from snapshot, carries forward unfetched articles
- Profile-scoped retention: context hash tags, filtered by active role
- Deferred briefing: scan completes immediately, briefing patches output async
- Model routing: 14b handles briefings (14-30s), 30b reserved for agent chat
- Two-pass scoring: fast timeout Pass 1, 3x slower Pass 2 for deferred items
- Serper + DDG dual provider with automatic fallback

### What's Not Yet Done

- **Cross-profile re-scoring** (Phase 2 from retention spec): when switching profiles, re-score other profiles' retained articles against the new context
- **Intelligence Archive** (Phase 3): dedicated searchable archive of all retained signals across all profiles
- **VRAM coexistence test**: scorer (8.7GB) + 14b (~9GB) may fit in 24GB simultaneously — haven't tested `OLLAMA_MAX_LOADED_MODELS=2` yet
- **Frontend SSE for briefing_ready**: backend broadcasts `briefing_ready` event, but frontend doesn't listen for it yet (briefing appears on next data poll, not instantly)
- **RSS feeds**: currently empty (`rss_feeds: []`), all news comes from search queries
- **Rotate compromised API keys** (Serper + Google) — exposed in public repo, scrubbed with git-filter-repo but keys should still be rotated
- **Update FRS v9 and README v8** to reflect model routing, deferred briefing, incremental scanning, profile-scoped retention

### What's Broken / Needs Attention

- **kuniv category over-scoring**: Articles mentioning "Kuwait" score high regardless of topical relevance. "Embassy of India, Kuwait" scored 9.0 for a petroleum professor. Scorer training issue, not pipeline bug.
- **Briefing export timing**: Briefing field in exported JSON may be empty if exported before the deferred briefing thread finishes. Not user-facing — dashboard shows the real briefing on next poll.

---

## 2. Decision Log

> Append-only. Each entry records WHAT was decided, WHY, and WHAT was rejected. Newest first.

### D008 — DoRA over standard LoRA (Feb 2026)
**Decision:** Use DoRA (Delta Orthogonal Rank Adaptation) for fine-tuning.
**Why:** DoRA offers 1-4% gains over LoRA with better tolerance for lower ranks. For a scoring task where small precision differences matter, this edge is worth the ~20% training overhead.
**Rejected:** Standard LoRA (marginally worse), full fine-tuning (won't fit in VRAM).

### D007 — CoT training over answer-only (Feb 2026)
**Decision:** Train scorer on structured reasoning traces inside `<think>` blocks, not just `SCORE: X | REASON: Y`.
**Why:** Answer-only training (v15-v18) produced flat scorers — models memorized patterns but couldn't reason about WHY an article matters. Profile awareness was 1.04 (essentially random). CoT training with V2 achieved 7.90 (7.6x improvement). Research confirms: Orca showed imitation models learn style not reasoning; only explanation tuning transfers reasoning capability.
**Rejected:** Answer-only format (v15-v18, insufficient), external CoT pipeline (unnecessary, Qwen3 has native `<think>` blocks).

### D006 — Incremental scanning via in-memory snapshot (Feb 26)
**Decision:** At scan start, snapshot previous `news_data.json`. After fetch, match URLs against snapshot. Known URLs reuse existing scores. Articles not re-fetched are carried forward to prevent output shrinkage.
**Why:** Back-to-back scans were re-scoring 100+ already-seen articles. Cut repeat scan time from ~10min to ~60s. Zero database changes — purely in-memory URL matching.
**Rejected:** Database-backed dedup with `scored_at` column (requires migration, more complex), ETag/Last-Modified on RSS (helps RSS only, not search). Context hash comparison ensures profile/role changes trigger full rescore.

### D005 — Deferred briefing via background thread (Feb 26)
**Decision:** Scan results appear immediately after scoring. Briefing generates in a daemon thread and patches output JSON when done. SSE `briefing_ready` event notifies frontend.
**Why:** Briefing was blocking scan completion for 2-5 minutes (with 30b) or 14-30s (with 14b). Users want to see scored articles immediately.
**Rejected:** Caching briefings (stale when top articles change), skipping briefings entirely (users value the summary).

### D004 — Profile-scoped retention via context hash (Feb 26)
**Decision:** Retained articles tagged with SHA-256 hash of `role|context|location` (lowercase, whitespace-normalized, 12-char truncation). Stored in `retained_by_profile` field in output JSON, not database.
**Why:** Profile name alone doesn't handle role switches within the same profile. Hashing all three fields ensures switching from "CE Student" to "Petroleum Professor" within "Ahmad" gives a clean feed. Normalization prevents hash changes from trivial edits.
**Rejected:** Profile name only (doesn't handle role switches), role-only hash (misses context/location), full role text (wasteful), database column (unnecessary — output JSON cycle is sufficient).

### D003 — Restrict 30b to agent + market only (Feb 26)
**Decision:** Briefings, suggestions, and summaries use 14b. Only strat agent chat and market analysis use 30b.
**Why:** 30b (18-19GB) evicts the scorer (8.7GB) from VRAM on a 24GB card. Every briefing triggered a 2-5 min model swap. 14b + scorer (~18GB total) can potentially coexist, eliminating swaps. Briefing quality with 14b is sufficient.
**Rejected:** Quantized scorer (saves VRAM but loses scoring quality), new `briefing_model` config key (same model as wizard, less config is better), keeping 30b for everything (swap penalty too high).

### D002 — 30b MoE over dense 14b for inference (Feb 2026)
**Decision:** Use `qwen3:30b-a3b` (MoE, 3.3B active) for strat agent.
**Why:** 80.4% AIME 2024 vs ~55% for 14b. 0.97 F1 on tool calling. Actually faster than dense 14b (~34 t/s vs ~32 t/s) because only 3.3B params active per token.
**Rejected:** Keeping 14b for everything (insufficient reasoning for agent tasks).

### D001 — Q8_0 quantization for scorer (Feb 2026)
**Decision:** Use Q8_0 (8.7GB) instead of Q4_K_M (~5GB) for fine-tuned scorer.
**Why:** Fine-tuned models are more sensitive to quantization — learned LoRA weight deltas are small and get disproportionately compressed. Q4_K_M introduced noticeable scoring errors.
**Rejected:** Q4_K_M (quality loss), Q6_K (acceptable but unnecessary given VRAM headroom).

---

## 3. Failure Log

> Non-obvious failures that future sessions might repeat. If the natural instinct would be to try the same approach, log it here.

### F007 — Qwen3-8B 8-bit loading fails
**What:** 8-bit quantized loading crashes. 4-bit works reliably.
**Rule:** Use 4-bit QLoRA for training (10-12GB VRAM). Use Q8_0 GGUF for inference (8.7GB).

### F006 — SFTTrainer `assistant_only_loss=True` breaks with Qwen3
**What:** Qwen3's default chat template lacks the `{% generation %}` keyword required by TRL's SFTTrainer.
**Rule:** Use modified template or Unsloth/ms-swift for loss masking.

### F005 — `think: false` doesn't work on Qwen3
**What:** Setting `"think": False` in Ollama API calls doesn't suppress thinking — model leaks reasoning as plain text instead of inside `<think>` blocks, consuming the entire `num_predict` budget.
**Rule:** Never set `think` parameter at all. Omit it entirely. Always strip `<think>...</think>` from output as safety net.

### F004 — Incremental scan dropped articles not re-fetched (Feb 26)
**What:** `_reuse_snapshot_scores()` only carried forward articles the fetcher re-fetched AND matched the snapshot. Articles not re-fetched (Serper dedup blocked the query) silently vanished. Output shrank 100 → 21.
**Fix:** Carry forward snapshot articles not in the fetch set (excluding retained, handled separately).
**Rule:** Incremental scan output must NEVER be smaller than previous output (barring dismissals or profile changes).

### F003 — Serper API key as YAML placeholder doesn't auto-expand (Feb 26)
**What:** `config.yaml` stored `${SERPER_API_KEY}` as literal string. YAML doesn't expand env vars. `_load_env_secrets()` was called once during `__init__` but not after config reloads in `run_scan()`/`run_news_refresh()`. Serper client received the literal string, got 401, silently fell back to DDG.
**Fix:** Added `_load_env_secrets()` after every config reload.
**Rule:** After any config reload, re-initialize secrets AND verify downstream clients see updated values.

### F002 — Answer-only training produces flat scorers (v15-v18)
**What:** Models trained on `SCORE: X | REASON: Y` (no reasoning traces) converge to giving everything 8.0-8.5. Profile awareness: 1.04 (random).
**Rule:** Always include `<think>` block reasoning traces in training data. V2 with CoT achieved 7.90 (7.6x improvement).

### F001 — AOTRITON causes NaN gradients on ROCm 6.2
**What:** `TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1` causes NaN during backward pass with DoRA + gradient checkpointing + bf16 on 7900 XTX.
**Rule:** Keep AOTRITON disabled for training. Safe for inference only.

### F000 — `_build_output()` whitelist drops unknown fields (Feb 26)
**What:** The output builder constructs each article dict from an explicit key list. New fields added to articles in-memory (like `retained_by_profile`) get silently dropped unless added to the whitelist. Cost a full debug cycle.
**Rule:** When adding article metadata, always check `_build_output()` in main.py.

### F-misc — Serper local tracker vs server-side credits can desync
**What:** `SerperQueryTracker` counts queries locally, but Serper tracks credits server-side independently. Old key showed 202 remaining locally but had zero credits on Serper's server.
**Rule:** Always trust the API response over local tracking. Use `set_remaining()` to sync.

### F-misc — PEFT meta device crash
**What:** After `get_peft_model()`, must delete `hf_device_map` and force `.to("cuda:0")` or gradient computation crashes.
**Rule:** Always do this cleanup step after PEFT model creation.

---

## 4. Session Log

> Brief summary of each session. Most recent first.

### Session: Feb 26, 2026 (Pipeline Optimizations + Retention)
**Commits:** 10 (9e42de4 → 39fc9a2)

1. **Serper provider fix:** `_load_env_secrets()` not called after config reload → silent DDG fallback. Fixed. Real root cause: old API key had zero credits server-side.
2. **Retention system:** Profile-scoped via context hash. Dismissal checks. Scan log tracking. Frontend "X kept" badge.
3. **Model routing:** 30b restricted to agent + market. Briefings use 14b. Eliminated 2-5min model swap.
4. **Deferred briefing:** Non-blocking. Articles appear immediately, briefing patches in via background thread + SSE.
5. **Incremental scanning:** Known URLs reuse snapshot scores. Carry-forward prevents output shrinkage. Repeat scans: ~60s vs ~10min.

**Pending:** Rotate compromised API keys. Update FRS v9 / README v8. Test VRAM coexistence (`OLLAMA_MAX_LOADED_MODELS=2`).

---

## 5. Hardware & Environment

| Component | Spec |
|-----------|------|
| CPU | AMD Ryzen 7 7800X3D |
| GPU (primary) | AMD Radeon 7900 XTX (24GB VRAM) |
| GPU (spare) | NVIDIA RTX 2070 Super 8GB (needs riser cable) |
| RAM | 32GB DDR5-6000 MT/s CL30 (dual-channel) |
| OS | Ubuntu 24 (dual-boot) |
| GPU Compute | AMD ROCm 6.2 |
| Python | 3.12 |
| Ollama | Local, `OLLAMA_MAX_LOADED_MODELS=1`, `OLLAMA_KEEP_ALIVE=10m` |

---

## 6. Training Data & Scorer History

| Version | Examples | Method | Cost | Key Metric | Outcome |
|---------|----------|--------|------|------------|---------|
| V1 (v15-v18) | 5,679 | Sonnet distillation, answer-only | ~$17 | Profile awareness: 1.04 | Failed — flat scoring |
| V2 (v19) | 20,550 | Opus Batch API, CoT traces, 30 profiles, 20 countries | ~$52 | Profile awareness: 7.90, Spearman ρ: 0.750 | **Production** |
| V3 (planned) | ~30,000 | Merged V1+V2 + new pipeline data | TBD | TBD | Not started |

---

## 7. Profiles

| Name | Role | Hash |
|------|------|------|
| Ahmad | Petroleum Engineering Professor / Seisnetics Consultant | `8e86611559e6` |
| saa | (incomplete/test profile) | — |

Primary profile is Computer Engineering student at AUK (Ahmad's real profile). Petroleum professor is active for testing.

---

## 8. Key File Locations

| What | Where |
|------|-------|
| Main orchestrator | `backend/main.py` |
| Scorer | `backend/processors/scorer_adaptive.py` |
| Scorer base/infra | `backend/processors/scorer_base.py` |
| Briefing generator | `backend/processors/briefing.py` |
| News fetcher | `backend/fetchers/news.py` |
| Kuwait intelligence | `backend/fetchers/kuwait_scrapers.py` |
| Serper client | `backend/fetchers/serper_search.py` |
| Database | `backend/database.py` |
| Migrations | `backend/migrations.py` |
| Auth/profiles | `backend/auth.py`, `backend/profiles/*.yaml` |
| Server/routes | `backend/server.py`, `backend/routes/` |
| Output JSON | `backend/output/news_data.json` |
| API keys | `backend/.env` (gitignored) |
| Codebase guide | `backend/CLAUDE.md` |
| This file | `STATE.md` (repo root) |

---

## Maintenance Notes

- **Claude Code:** At session end, run "Update STATE.md with this session's changes"
- **Claude.ai:** Generate updated STATE.md content for manual commit or Project Knowledge upload
- **Decision Log:** Append-only — never edit old entries, add new ones
- **Failure Log:** Curated — only non-obvious failures worth preventing
- **Session Log:** 3-5 lines per session, most recent first
